{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\JiYongHa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import openpyxl\n",
    "import nltk\n",
    "import time\n",
    "from openpyxl import load_workbook\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer #감정분석\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize #토큰 생성\n",
    "from nltk.tag import pos_tag # 형태소 태그생성\n",
    "from nltk import FreqDist #빈도수 측정\n",
    "from wordcloud import WordCloud #워드 클라우드\n",
    "import matplotlib.pyplot as plt\n",
    "import re #정규식 사용\n",
    "from datetime import datetime, timedelta #시간계산 datetime(날짜), time,delta(시간의 차)\n",
    "from pytz import timezone #시간대 변경\n",
    "from googletrans import Translator#구글 번역\n",
    "from langdetect import detect#언어감지\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "from json import JSONDecodeError\n",
    "\n",
    "nltk.download(\"book\")\n",
    "nltk.download('vader_lexicon')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "endtype = {1 : \"success\", 2:'fail', 3:'no change'}\n",
    "\n",
    "def open_workbook(file = None): \n",
    "  workbook = load_workbook(file, data_only=True)#엑셀 열기\n",
    "  return workbook\n",
    "\n",
    "def clean_str(text):\n",
    "  try:\n",
    "    pattern = '([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)' # E-mail제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)  #string에서 pattern과 매치하는 텍스트를 repl로 치환한다\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+' # URL제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '<[^>]*>'         # HTML 태그 제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    #pattern = re.compile(re.escape('(Translated by Google)')+'.*') # 구글 뒤 처리\n",
    "    #text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    #pattern = re.compile(re.escape('(Google 번역)')+'.*') # 구글 뒤 처리\n",
    "    #text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    #pattern = '[^\\w\\s]'         # 특수기호제거\n",
    "    #text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    ##패턴 문자열 pattern을 패턴 객체로 컴파일한다\n",
    "    pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    #text = re.compile(re.escape('((Google 번역))')+'.*')\n",
    "    #text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    #text = re.compile(re.escape('(Translated by Google)')+'.*')\n",
    "    #text = re.sub(pattern=pattern, repl='', string=text)\n",
    "  except TypeError:\n",
    "    test = 'neutral'\n",
    "  return text     \n",
    "\n",
    "def remwithre(text, there=re.compile(re.escape('(Google 번역)'))):\n",
    "    return there.sub('', str(text))\n",
    "def remwithre2(text, there=re.compile(re.escape('(원본)')+'.*')):\n",
    "    return there.sub('', str(text))\n",
    "def remwithre3(text, there=re.compile(re.escape('(Google 번역)')+'.*')):\n",
    "    return there.sub('', text)\n",
    "\n",
    "def get_comments(sheet,startrow=1) :\n",
    "  result = []\n",
    "  row_num = sheet.max_row#시트 줄수\n",
    "  col_num = sheet.max_column#시트 열수\n",
    "\n",
    "  if row_num<startrow:\n",
    "    return result\n",
    "\n",
    "  for r in range(startrow, row_num+1) :\n",
    "    clean_comment = clean_str(sheet.cell(r,3).value)\n",
    "    clean_comment = remwithre(clean_comment)\n",
    "    clean_comment = remwithre2(clean_comment)\n",
    "    clean_comment = remwithre3(clean_comment)\n",
    "    if clean_comment is None: #empty set\n",
    "      clean_comment = 'neutral'\n",
    "    result.append(clean_comment)\n",
    "\n",
    "  return result\n",
    "\n",
    "def get_translate(comments = None):\n",
    "  result = []\n",
    "  for i in range(0,len(comments)) :\n",
    "    try:\n",
    "      translator = Translator(service_urls=[\n",
    "      'translate.google.com',\n",
    "      'translate.google.co.kr',\n",
    "      'translate.google.co.jp',\n",
    "      'translate.google.co.uk',\n",
    "      ])\n",
    "\n",
    "      #lang = detect(comments[i])\n",
    "      #print(i,translator.detect(comments[i]).lang, comments[i])\n",
    "      if(translator.detect(comments[i]).lang == 'en'):\n",
    "        result.append(comments[i])\n",
    "      else:\n",
    "        time.sleep(1)\n",
    "        trs_comment = translator.translate(comments[i])\n",
    "        print(comments[i], \"\\ntranslated : \",trs_comment.text)\n",
    "        result.append(trs_comment.text)\n",
    "    except (LangDetectException, AttributeError, TypeError):\n",
    "      result.append('neutral')\n",
    "    except JSONDecodeError:\n",
    "      print(\"JSONERROR :\", comments[i])\n",
    "      result.append(comments[i])\n",
    "  \n",
    "  return result\n",
    "\n",
    "def analyze_emotion(trans_list = None) :#점수 매기기\n",
    "  result = []\n",
    "  for i in trans_list:\n",
    "    score = sid.polarity_scores(i)\n",
    "    result.append(score)\n",
    "    \n",
    "  return result\n",
    "\n",
    "\n",
    "def get_like(sheet = None, startrow = 1) : #int + 답변의 내용에서 좋아요 숫자를 구하기(정규표현식)\n",
    "  desc = re.compile('\\d*\\d')\n",
    "  result = []\n",
    "\n",
    "  for i in range(startrow,sheet.max_row+1) :\n",
    "    find_like = desc.findall(sheet.cell(i,4).value)\n",
    "\n",
    "    if len(find_like) is None:\n",
    "      result.append('0')\n",
    "    else :\n",
    "      #print(find_like)\n",
    "      if len(find_like) == 1 :\n",
    "        result.append('0')\n",
    "      elif len(find_like) == 2:\n",
    "        #print(int(find_like[1])-1)\n",
    "        result.append(str(int(find_like[1])-1))            \n",
    "  return result\n",
    "\n",
    "\n",
    "\n",
    "def get_realtime(sheet = None, startrow = 1):#시간계산\n",
    "  #년,달,주,일,시간,분\n",
    "\n",
    "  minute_desc = re.compile('\\d*\\d분')\n",
    "  hour_desc = re.compile('\\d*\\d시간')\n",
    "  day_desc = re.compile('\\d*\\d일')\n",
    "  week_desc = re.compile('\\d*\\d주')\n",
    "  month_desc = re.compile('\\d*\\개월')\n",
    "  year_desc = re.compile('\\d*\\d년')\n",
    "  desc = re.compile('\\d*\\d')\n",
    "\n",
    "  #시간 출력 포맷\n",
    "  format = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "  #현재시간\n",
    "  UTC = datetime.now(timezone('UTC'))\n",
    "  KST = datetime.now(timezone('Asia/Seoul'))\n",
    "\n",
    "  #print(KST)\n",
    "\n",
    "  result = []\n",
    "  \n",
    "  # ~전 토큰으로 자름 return객체 = list\n",
    "    \n",
    "  for i in range(startrow,sheet.max_row+1) :\n",
    "    minute_ago = minute_desc.findall(sheet.cell(i,2).value)\n",
    "    hour_ago = hour_desc.findall(sheet.cell(i,2).value)\n",
    "    day_ago = day_desc.findall(sheet.cell(i,2).value)\n",
    "    week_ago = week_desc.findall(sheet.cell(i,2).value)\n",
    "    month_ago = month_desc.findall(sheet.cell(i,2).value)\n",
    "    year_ago = year_desc.findall(sheet.cell(i,2).value)\n",
    "    #print(minute_ago,hour_ago,day_ago,week_ago,month_ago,year_ago)\n",
    "    \n",
    "    # 길이 != 0 -> 현재시간 - 차이 시간 = return객체 = datetime -> str으로 변경\n",
    "    if len(minute_ago) != 0:\n",
    "      min_out = int((desc.findall(minute_ago[0]))[0])\n",
    "      rtime = KST-timedelta(minutes = min_out)\n",
    "      str_rtime = rtime.strftime(format)\n",
    "      #print(str_rtime)\n",
    "      result.append(str_rtime)\n",
    "      \n",
    "    elif len(hour_ago) != 0:\n",
    "      hour_out = int((desc.findall(hour_ago[0]))[0])\n",
    "      rtime = KST-timedelta(hours = hour_out)\n",
    "      str_rtime = rtime.strftime(format)\n",
    "      #print(str_rtime)\n",
    "      result.append(str_rtime)\n",
    "      \n",
    "    elif len(day_ago) != 0:\n",
    "      day_out = int((desc.findall(day_ago[0]))[0])\n",
    "      rtime = KST-timedelta(days = day_out)\n",
    "      str_rtime = rtime.strftime(format)\n",
    "      #print(str_rtime)\n",
    "      result.append(str_rtime)\n",
    "\n",
    "    elif len(week_ago) != 0:\n",
    "      week_out = int((desc.findall(week_ago[0]))[0])\n",
    "      rtime = KST-timedelta(days = week_out*7)\n",
    "      str_rtime = rtime.strftime(format)\n",
    "      #print(str_rtime)\n",
    "      result.append(str_rtime)\n",
    "\n",
    "    elif len(month_ago) != 0:\n",
    "      month_out = int((desc.findall(month_ago[0]))[0])\n",
    "      rtime = KST-timedelta(days = month_out*30)\n",
    "      str_rtime = rtime.strftime(format)\n",
    "      #print(str_rtime)\n",
    "      result.append(str_rtime)\n",
    "\n",
    "    elif len(year_ago) != 0:\n",
    "      year_out = int((desc.findall(year_ago[0]))[0])\n",
    "      rtime = KST-timedelta(days = year_out*365)\n",
    "      str_rtime = rtime.strftime(format)\n",
    "      #print(str_rtime)\n",
    "      result.append(str_rtime)\n",
    "    \n",
    "    else :\n",
    "      result.append(KST.strftime(format))\n",
    "\n",
    "  return result\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def add_score2excel(file = None,sheet = None,startrow=1): #파일명, 시작row\n",
    "  if file is None and sheet is None:\n",
    "    return endtype[2]\n",
    "  if startrow < 1 :\n",
    "    startrow = 1\n",
    "  \n",
    "  book = open_workbook(file)\n",
    "  sheet1 = book[sheet]\n",
    "  \n",
    "  comment_list = get_comments(sheet1,startrow)\n",
    "  trans_list = get_translate(comment_list)# 번역된 리스트\n",
    "  score_list = analyze_emotion(trans_list) # 점수 리스트\n",
    "  #like_list = get_like(sheet1,startrow) # like 수 리스트\n",
    "  #time_list = get_realtime(sheet1,startrow) # 절대 시간 리스트\n",
    "\n",
    "  #print(len(comment_list),len(trans_list),len(score_list),len(like_list),len(time_list))\n",
    "\n",
    "  if(len(score_list)==0):\n",
    "      return endtype[3]\n",
    "  sheet1_row = sheet1.max_row\n",
    "\n",
    "  writerow = startrow#점수 열에 추가하기 D E F G\n",
    "  for i in range(0,len(score_list)):\n",
    "    #print(writerow ,score_list[i])\n",
    "    #sheet1.cell(writerow,2).value = time_list[i]\n",
    "    sheet1.cell(writerow,3).value = trans_list[i] #번역\n",
    "    #sheet1.cell(writerow,5).value = like_list[i]\n",
    "    sheet1.cell(writerow,4).value = score_list[i]['neg'] #부정\n",
    "    sheet1.cell(writerow,5).value = score_list[i]['neu'] #중립\n",
    "    sheet1.cell(writerow,6).value = score_list[i]['pos'] #긍정\n",
    "    sheet1.cell(writerow,7).value = score_list[i]['compound'] #복합\n",
    "    writerow+=1\n",
    "    #emotion_list = [i,[score_list[i]['neg'], score_list[i]['neu'], score_list[i]['pos'], score_list[i]['compound']]]\n",
    "    #print(writerow ,score_list[i])\n",
    "    \n",
    "    \n",
    "  book.save(file)\n",
    "  return endtype[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "무료입장이라는게 신기할정도로 공원이 크고 동물도 많았어요 개인적으로 5천원이하 돈낸다해도 갈것같아요 :) 동물들이 많아서 주차할때부터 냄새는 많이 났지만 무료에 이정도 수준이라..놀랬어요 좋아요~^^ \n",
      "translated :  I twenty won lots of big animals enough to wear yiraneunge free admission Personally I think less than 5,000 won galgeot be paying for it :) animals since the parking lot smells natjiman much as this level for free ... I freaking love it ~ ^^\n",
      "무료입장에 주차비 4천원만 내면되서 좋다.. 동물이 다양하나..관리가 안되는지 냄새가 심하다 하지만 서문시장구경하고 아이들과 잠깐 시간보내기에는 괜찮다.. \n",
      "translated :  Free parking doeseo good position, only 4,000 won inside .. one .. various animals severe to manage the andoeneunji smell fine, but has visited the preface market and children and send some time.\n",
      "오랜새월이 흘러서갔는데 깨끗하구. 동물도있고 꼭한번쯤가보세요. 후회하지않아요 \n",
      "translated :  I went for a long saewol estuary flows clean. Animals, head and come at least once. Do not regret\n",
      "공원이 잘정리되어있고 입장료가 무료입니딘 동물들도 많아 애들이랑 가기 좋아요 주차요금은 2시간에 5천원이구요, 동물들땜에 냄새가 조금나는것빼구는 완전좋아요 \n",
      "translated :  Things ipni Dean Park is well organized and admission is free animals is also good with the kids more top parking fee slip is 5,000 won in 2 hours, the smell a bit distressed animals ppaegu I completely love\n",
      "아이들이 놀기 좋아요. 동물들도 무료로 볼수 있고요. 무료이지만 동물 종류가 꽤 많아요. 호랑이, 곰, 코끼리 까지 있어요. 주차장이 없지만 주변에 유료주차장들이 많아서 괜찮습니다. 일찍가거나 운이 좋으면 공영유료주차장에 주차할 수 있어요. \n",
      "translated :  I love playing with children. Itgoyo also see free animals. Free, but quite a lot animal species. Tiger, it's a bear, an elephant up. Although the parking lot was their paid parking around fine. You can go early and get lucky on public pay parking garage.\n",
      "동물이 불쌍하다.. 세기말 동물원 느낌. 막 탈출할 것 같음. 그런 면에서는 가볼 만함 \n",
      "translated :  It is the animal feeling sorry .. century zoo. Same film will escape. In that sense, go manham\n",
      "무료라서 부담없이 갈 수잇고. 단점으론 동물냄새가 좀 심하다는거. 물론 동물원이라 그렇지만. 바람새면서 동물구경도하고. \n",
      "translated :  Free because itgo go willingly. Disadvantages euron're an animal that smells a little harsh. But as well as the zoo. As new winds and even animals around.\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "#testline\n",
    "\n",
    "#file = \"C:\\datasource\\Samsung.xlsx\"\n",
    "file = \"달성공원_구글맵.xlsx\"\n",
    "file2 = \"달성공원_구글맵.xlsx\"\n",
    "sheet1 = 'Sheet1'\n",
    "\n",
    "#sheet2 = '2'\n",
    "#sheet1 = '1'\n",
    "#filename = \"TripAdvisor_reply.xlsx\"\n",
    "#book = openpyxl.load_workbook(filename)\n",
    "\n",
    "book = open_workbook(file2)\n",
    "\n",
    "#sheet1 = book[sheet]#\n",
    "\n",
    "#############################################\n",
    "\n",
    "#print(add_score2excel(file,sheet,1))\n",
    "\n",
    "print(add_score2excel(file2,sheet1,1))\n",
    "\n",
    "\n",
    "#noun_list = make_wcloud(file,'Sheet1')\n",
    "#noun_list = make_wcloud(file2,'Sheet1')\n",
    "\n",
    "############################################\n",
    "\n",
    "#result = get_comments(sheet1,1070)\n",
    "#print(result,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
